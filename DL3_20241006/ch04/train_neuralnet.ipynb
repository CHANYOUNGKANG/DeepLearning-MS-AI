{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# coding: utf-8\n",
        "import os, sys\n",
        "import numpy as np\n",
        "sys.path.append(os.path.abspath(\"..\"))\n",
        "from dataset.mnist import load_mnist\n",
        "from ch04.two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 1000000  # 학습 횟수 10000번 지정\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 1000  # 배치(묶음) 크기 100개로 지정\n",
        "learning_rate = 0.1  # 학습률 0.1로 지정\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)  # 오차역전파법 방식\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.09488333333333333 0.0962\n0.5369 0.5395\n0.7745333333333333 0.7842\n0.8410833333333333 0.8482\n0.8712833333333333 0.8768\n0.8844666666666666 0.8904\n0.8914333333333333 0.8966\n0.8985 0.9018\n0.9020666666666667 0.9055\n0.9059 0.9082\n0.9088833333333334 0.9116\n0.9111 0.9135\n0.9128166666666667 0.9162\n0.9155 0.9171\n0.9167666666666666 0.9197\n0.9192833333333333 0.9193\n0.91995 0.9218\n0.9212666666666667 0.923\n0.9237833333333333 0.9245\n0.9241833333333334 0.9253\n0.9258333333333333 0.9276\n0.92725 0.9285\n0.92805 0.9282\n0.9296 0.9293\n0.9302666666666667 0.9301\n0.9311166666666667 0.9314\n0.9326333333333333 0.9321\n0.93385 0.9322\n0.9345333333333333 0.934\n0.9355 0.9347\n0.9364333333333333 0.9359\n0.9372666666666667 0.9369\n0.9387333333333333 0.9369\n0.9394666666666667 0.9379\n0.9402333333333334 0.9391\n0.94055 0.9383\n0.9411333333333334 0.9388\n0.9424166666666667 0.9395\n0.9435333333333333 0.9412\n0.94375 0.9417\n0.9448 0.9423\n0.9449666666666666 0.9437\n0.94615 0.9433\n0.9468833333333333 0.9431\n0.9478833333333333 0.9435\n0.9481833333333334 0.9444\n0.9488333333333333 0.9463\n0.9494 0.9451\n0.9501166666666667 0.9449\n0.9511166666666667 0.9465\n0.9508 0.9459\n0.9519 0.9474\n0.9520333333333333 0.9486\n0.9525833333333333 0.9479\n0.9530166666666666 0.9492\n0.9535666666666667 0.95\n0.9540666666666666 0.9496\n0.9547666666666667 0.9503\n0.9552833333333334 0.9504\n0.9555333333333333 0.9505\n0.9560833333333333 0.9517\n0.9565833333333333 0.9519\n0.95665 0.9521\n0.9575166666666667 0.9528\n0.9579833333333333 0.9538\n0.9578333333333333 0.9531\n0.9585666666666667 0.9542\n0.9594 0.9553\n0.95955 0.9556\n0.96015 0.9556\n0.9602666666666667 0.9562\n0.9606 0.9566\n0.9615166666666667 0.9568\n0.9614833333333334 0.9565\n0.96225 0.9571\n0.96155 0.957\n0.9618333333333333 0.9581\n0.96265 0.9576\n0.96305 0.958\n0.96355 0.9578\n0.96375 0.9585\n0.9641333333333333 0.9587\n0.96445 0.9588\n0.9651 0.9589\n0.9652833333333334 0.9596\n0.9653166666666667 0.9595\n0.9656666666666667 0.9599\n0.9658833333333333 0.9595\n0.9664 0.9604\n0.9664166666666667 0.9602\n0.9669333333333333 0.9618\n0.96725 0.9618\n0.9675 0.961\n0.9676333333333333 0.9615\n0.9680666666666666 0.9617\n0.9681333333333333 0.9623\n0.9687333333333333 0.9623\n0.9687166666666667 0.9626\n0.9691666666666666 0.9631\n0.969 0.9631\n0.9697333333333333 0.9633\n0.97025 0.9638\n0.9701166666666666 0.964\n0.9702166666666666 0.9631\n0.9704166666666667 0.9645\n0.9705833333333334 0.9637\n0.9709333333333333 0.9644\n0.9711833333333333 0.9649\n0.9712833333333334 0.9644\n0.9719833333333333 0.9659\n0.972 0.9645\n0.97235 0.965\n0.9724 0.9643\n0.9723166666666667 0.9651\n0.9725 0.9651\n0.9730666666666666 0.9651\n0.9727666666666667 0.9658\n0.9730833333333333 0.9664\n0.9734166666666667 0.9657\n0.97355 0.9655\n0.9738833333333333 0.9661\n0.9741166666666666 0.9664\n0.9744166666666667 0.9671\n0.9742833333333333 0.9669\n0.9747666666666667 0.9661\n0.9749833333333333 0.9664\n0.9748666666666667 0.9671\n0.9752 0.9663\n0.9754 0.9665\n0.9756666666666667 0.9672\n0.9756833333333333 0.9669\n0.9756333333333334 0.9679\n0.9754666666666667 0.9679\n0.9762666666666666 0.9678\n0.9763 0.9681\n0.9765333333333334 0.9677\n0.9765666666666667 0.968\n0.9765833333333334 0.9685\n0.9767 0.9681\n0.977 0.9679\n0.9772333333333333 0.9685\n0.9772 0.9686\n0.9774333333333334 0.969\n0.9777166666666667 0.968\n0.9778666666666667 0.9696\n0.9782833333333333 0.9682\n0.9779333333333333 0.9688\n0.9784166666666667 0.9691\n0.9781166666666666 0.969\n0.97865 0.969\n0.9785833333333334 0.9692\n0.9786 0.9697\n0.97865 0.9691\n0.9787 0.9694\n0.97885 0.969\n0.9791166666666666 0.9693\n0.9792166666666666 0.9693\n0.9792166666666666 0.9697\n0.9798833333333333 0.9695\n0.9797 0.9696\n0.9798 0.9699\n0.9796333333333334 0.97\n0.98025 0.9703\n0.9800333333333333 0.9698\n0.9803666666666667 0.9703\n0.9806 0.9703\n0.9802833333333333 0.9715\n0.9807166666666667 0.9698\n0.981 0.971\n0.9812166666666666 0.9703\n0.98135 0.9711\n0.98095 0.9714\n0.9812333333333333 0.9704\n0.98135 0.972\n0.9813 0.9716\n0.98155 0.9713\n0.9815833333333334 0.9709\n0.98175 0.9712\n0.9818666666666667 0.9714\n0.9817 0.9716\n0.9823666666666667 0.9709\n0.98225 0.9717\n0.9822333333333333 0.9723\n0.9822333333333333 0.9715\n0.98245 0.9717\n0.9826166666666667 0.9716\n0.9826166666666667 0.972\n0.9830333333333333 0.9717\n0.9824833333333334 0.9719\n0.9827166666666667 0.972\n0.98275 0.9719\n0.983 0.9729\n0.9829833333333333 0.9725\n0.9828666666666667 0.9714\n0.9829833333333333 0.9717\n0.9834333333333334 0.9725\n0.9834666666666667 0.9726\n0.9831833333333333 0.9725\n0.9833833333333334 0.9716\n0.9835833333333334 0.9728\n0.9838666666666667 0.9721\n0.9836166666666667 0.9719\n0.9840833333333333 0.9725\n0.9842166666666666 0.9721\n0.9838 0.9721\n0.9842333333333333 0.9724\n0.9840833333333333 0.9718\n0.9844166666666667 0.9724\n0.9842166666666666 0.9722\n0.9838166666666667 0.9722\n0.9843 0.972\n0.9844166666666667 0.9729\n0.9844166666666667 0.9729\n0.9845666666666667 0.9722\n0.98455 0.973\n0.9848333333333333 0.9723\n0.9847166666666667 0.9727\n0.9850166666666667 0.973\n0.9849333333333333 0.9731\n0.9848333333333333 0.9728\n0.9853666666666666 0.9731\n0.9847333333333333 0.9723\n0.9851166666666666 0.9731\n0.9852 0.9732\n0.9853833333333334 0.9731\n0.9854333333333334 0.9728\n0.9856666666666667 0.9729\n0.9856666666666667 0.9728\n0.9854166666666667 0.9727\n0.9858166666666667 0.9729\n0.9858166666666667 0.973\n0.9857333333333334 0.9738\n0.9861166666666666 0.9733\n0.9861666666666666 0.9729\n0.98615 0.9738\n0.9858833333333333 0.9733\n0.9861333333333333 0.9731\n0.98625 0.9729\n0.9863 0.9731\n0.98625 0.9734\n0.9865 0.9729\n0.9865833333333334 0.973\n0.9864833333333334 0.9732\n0.98665 0.9736\n0.9869333333333333 0.9732\n0.9866166666666667 0.9731\n0.9869 0.9739\n0.9867833333333333 0.9736\n0.9866833333333334 0.9734\n0.9869 0.9741\n0.9871166666666666 0.9732\n0.98725 0.9736\n0.9869666666666667 0.9734\n0.9871666666666666 0.9734\n0.9872 0.9737\n0.9871166666666666 0.9747\n0.9874166666666667 0.974\n0.9875833333333334 0.9734\n0.9875333333333334 0.9738\n0.9875333333333334 0.9737\n0.9877666666666667 0.9734\n0.9877666666666667 0.9736\n0.98765 0.9738\n0.9876166666666667 0.974\n0.9874833333333334 0.9734\n0.9878666666666667 0.9733\n0.9879833333333333 0.9735\n0.9878666666666667 0.9736\n0.9881333333333333 0.9731\n0.9880833333333333 0.9738\n0.9881333333333333 0.9735\n0.9882666666666666 0.9738\n0.9881833333333333 0.974\n0.9883833333333333 0.974\n0.9883666666666666 0.9737\n0.98815 0.9739\n0.9882166666666666 0.9741\n0.9884166666666667 0.9735\n0.9887 0.9738\n0.9887 0.9741\n0.9889 0.9739\n0.9889666666666667 0.9738\n0.98905 0.9742\n0.9889166666666667 0.9739\n0.9890333333333333 0.9741\n0.9888833333333333 0.9739\n0.9888833333333333 0.9745\n0.9891833333333333 0.9745\n0.98915 0.974\n0.98925 0.974\n0.98915 0.9737\n0.9892 0.9742\n0.9892 0.9741\n0.9893833333333333 0.9739\n0.98935 0.974\n0.98945 0.9739\n0.9895833333333334 0.9737\n0.9895833333333334 0.974\n0.9892333333333333 0.9733\n0.9897666666666667 0.9739\n0.9898166666666667 0.9735\n0.9897833333333333 0.9741\n0.9896333333333334 0.9742\n0.98965 0.974\n0.98995 0.9739\n0.9899333333333333 0.9742\n0.9899333333333333 0.9739\n0.9901 0.9741\n0.9899 0.9737\n0.9904666666666667 0.9737\n0.9900333333333333 0.974\n0.9903166666666666 0.9742\n0.9902166666666666 0.9739\n0.99 0.9743\n0.9903 0.9737\n0.99005 0.9745\n0.9905 0.9738\n0.9902333333333333 0.9739\n0.9906 0.9742\n0.9904166666666666 0.9739\n0.9903333333333333 0.9739\n0.9904 0.9742\n0.9906666666666667 0.9742\n0.99075 0.9744\n0.9906833333333334 0.9743\n0.9908166666666667 0.9741\n0.9908833333333333 0.9744\n0.9908333333333333 0.9744\n0.9906666666666667 0.9742\n0.99105 0.9747\n0.9906666666666667 0.9743\n0.9907666666666667 0.9739\n0.9909 0.9744\n0.9909666666666667 0.9744\n0.9909833333333333 0.9744\n0.99115 0.9743\n0.9914 0.974\n0.9911833333333333 0.9749\n0.99135 0.974\n0.9911166666666666 0.9743\n0.9913666666666666 0.9746\n0.9913166666666666 0.974\n0.9914166666666666 0.9747\n0.9917166666666667 0.9746\n0.9914833333333334 0.974\n0.9913666666666666 0.9743\n0.9917666666666667 0.9749\n0.9916666666666667 0.9747\n0.9915 0.975\n0.9916666666666667 0.9746\n0.9916333333333334 0.9743\n0.9917833333333334 0.9748\n0.99155 0.9743\n0.9919 0.9741\n0.9920166666666667 0.9742\n0.9918 0.9743\n0.9920833333333333 0.9739\n0.992 0.974\n0.9919166666666667 0.9738\n0.992 0.9745\n0.9922166666666666 0.9745\n0.9921833333333333 0.9747\n0.9923 0.9743\n0.9924333333333333 0.9747\n0.9923333333333333 0.9743\n0.9922333333333333 0.9743\n0.9920166666666667 0.9747\n0.9924166666666666 0.9745\n0.9924333333333333 0.9748\n0.9923833333333333 0.975\n0.9924 0.9744\n0.99245 0.9743\n0.9925 0.9748\n0.9923166666666666 0.9747\n0.9926666666666667 0.9743\n0.9926666666666667 0.9737\n0.9926333333333334 0.9741\n0.9925833333333334 0.9745\n0.9929666666666667 0.9751\n0.9926666666666667 0.9745\n0.9930166666666667 0.9745\n0.99275 0.9746\n0.9929666666666667 0.9746\n0.9929666666666667 0.975\n0.9929166666666667 0.9751\n0.9929166666666667 0.9746\n0.9929666666666667 0.9751\n0.9928833333333333 0.9749\n0.99335 0.975\n0.9934666666666667 0.9748\n0.9933333333333333 0.9746\n0.99335 0.9746\n0.9935333333333334 0.9748\n0.9935166666666667 0.9747\n0.9933333333333333 0.9745\n0.9935166666666667 0.9743\n0.9932666666666666 0.9744\n0.9937333333333334 0.9743\n0.9936666666666667 0.9745\n0.9936166666666667 0.9758\n0.9936166666666667 0.975\n0.9939 0.9747\n0.9939666666666667 0.9745\n0.9938333333333333 0.9746\n0.99365 0.9753\n0.9937666666666667 0.9749\n0.9936833333333334 0.9751\n0.9938666666666667 0.9754\n0.9938833333333333 0.9753\n0.9937 0.9749\n0.9939 0.9758\n0.99385 0.9753\n0.9936833333333334 0.9752\n0.99405 0.9748\n0.9941166666666666 0.975\n0.9940833333333333 0.9747\n0.9941666666666666 0.9748\n0.9942 0.975\n0.9940833333333333 0.9749\n0.9942 0.9752\n0.99375 0.9749\n0.9943333333333333 0.9751\n0.99435 0.9752\n0.9942666666666666 0.9751\n0.9943833333333333 0.9754\n0.9942833333333333 0.9754\n0.9946666666666667 0.9755\n0.9946166666666667 0.9757\n0.9945166666666667 0.9749\n0.9946333333333334 0.9762\n0.9945 0.9753\n0.9945666666666667 0.9755\n0.9945833333333334 0.9759\n0.9946333333333334 0.9757\n0.9947333333333334 0.9754\n0.9947666666666667 0.9756\n0.9948333333333333 0.9759\n0.9946 0.9757\n0.9949 0.9757\n0.9948833333333333 0.9754\n0.995 0.9756\n0.995 0.9758\n0.99485 0.9756\n0.995 0.9753\n0.9949666666666667 0.9746\n0.9948333333333333 0.9749\n0.9952166666666666 0.9762\n0.9948333333333333 0.9756\n0.9950833333333333 0.9751\n0.9951666666666666 0.9761\n0.9951333333333333 0.9758\n0.9950166666666667 0.9751\n0.9950333333333333 0.9753\n0.9951666666666666 0.9752\n0.99535 0.9755\n0.9952 0.9762\n0.9953166666666666 0.976\n0.9955166666666667 0.9756\n0.9952166666666666 0.9753\n0.9953166666666666 0.9758\n0.9953666666666666 0.9758\n0.9952666666666666 0.976\n0.9954 0.9758\n0.9953833333333333 0.9756\n0.9954333333333333 0.9751\n0.9953666666666666 0.9756\n0.9954833333333334 0.9755\n0.9954666666666667 0.976\n0.9957333333333334 0.9765\n0.9956333333333334 0.9756\n0.99555 0.9755\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     36\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grad[key]\n\u001b[0;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m iter_per_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/b001-dl/code/Users/5b001/DL3_20241006/ch04/two_layer_net.py:37\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[0;32m---> 37\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastLayer\u001b[38;5;241m.\u001b[39mforward(y, t)\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/b001-dl/code/Users/5b001/DL3_20241006/ch04/two_layer_net.py:32\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m---> 32\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/b001-dl/code/Users/5b001/DL3_20241006/common/layers.py:57\u001b[0m, in \u001b[0;36mAffine.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 57\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1729737264702
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}